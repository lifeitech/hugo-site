<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>diffusion on lifei.ai | AI blogs</title>
    <link>https://lifei.ai/tags/diffusion/</link>
    <description>Recent content in diffusion on lifei.ai | AI blogs</description>
    <image>
      <title>lifei.ai | AI blogs</title>
      <url>https://lifei.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://lifei.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 12 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://lifei.ai/tags/diffusion/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Vision Models</title>
      <link>https://lifei.ai/posts/2024-02-12-vision-models/</link>
      <pubDate>Mon, 12 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://lifei.ai/posts/2024-02-12-vision-models/</guid>
      <description>At the same time LLMs are changing the world, we have witnessed fast progress in AI image and video generation. AI models are now able to synthesis high quality, high fidelity and high resolution images via text prompting. Mainstream generative model architectures have shifted from VAEs, flows and GANs to diffusions and transformers. In this post I take notes of several vision models. It may be regularly updated to reflect latest research development.</description>
    </item>
  </channel>
</rss>
