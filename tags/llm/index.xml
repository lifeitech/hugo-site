<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llm on lifei.ai | AI blogs</title>
    <link>https://lifei.ai/tags/llm/</link>
    <description>Recent content in llm on lifei.ai | AI blogs</description>
    <image>
      <title>lifei.ai | AI blogs</title>
      <url>https://lifei.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://lifei.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 31 Aug 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://lifei.ai/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Better Transformers</title>
      <link>https://lifei.ai/posts/2023-08-31-better-transformers/</link>
      <pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://lifei.ai/posts/2023-08-31-better-transformers/</guid>
      <description>In this post I will walk through the transformer layer and several improvements over this architecture that are commonly employed in many popular open source large language models (LLMs) today, for example Llama. Discussed include SwiGLU and RMSNorm layers, RoPE and ALiBi position embeddings; and finally Flash Attention for scaling attention calculation to long sequences. We will use Llama source code as example implementation, and toward the end I&amp;rsquo;ll go through the rest of Llama&amp;rsquo;s source code.</description>
    </item>
  </channel>
</rss>
