<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>transformer on lifei.ai | AI blogs</title>
    <link>https://lifei.ai/tags/transformer/</link>
    <description>Recent content in transformer on lifei.ai | AI blogs</description>
    <image>
      <title>lifei.ai | AI blogs</title>
      <url>https://lifei.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://lifei.ai/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 12 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://lifei.ai/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Vision Models</title>
      <link>https://lifei.ai/posts/2024-02-12-vision-models/</link>
      <pubDate>Mon, 12 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://lifei.ai/posts/2024-02-12-vision-models/</guid>
      <description>At the same time LLMs are changing the world, we have witnessed fast progress in AI image and video generation. AI models are now able to synthesis high quality, high fidelity and high resolution images via text prompting. Mainstream generative model architectures have shifted from VAEs, flows and GANs to diffusions and transformers. In this post I take notes of several vision models. It may be regularly updated to reflect latest research development.</description>
    </item>
    <item>
      <title>Better Transformers</title>
      <link>https://lifei.ai/posts/2023-08-31-better-transformers/</link>
      <pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://lifei.ai/posts/2023-08-31-better-transformers/</guid>
      <description>In this post I will walk through the transformer layer and several improvements over this architecture that are commonly employed in many popular open source large language models (LLMs) today, for example Llama. Discussed include SwiGLU and RMSNorm layers, RoPE and ALiBi position embeddings; and finally Flash Attention for scaling attention calculation to long sequences. We will use Llama source code as example implementation, and toward the end I&amp;rsquo;ll go through the rest of Llama&amp;rsquo;s source code.</description>
    </item>
  </channel>
</rss>
